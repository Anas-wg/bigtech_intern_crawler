from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
from .base_crawler import BaseCrawler

class DaangnCrawler(BaseCrawler):
    def __init__(self):
        super().__init__("https://about.daangn.com/jobs")

        # ğŸ”¹ í¬ë¡¤ë§í•  ì§êµ°ë³„ ì¸í„´ ê³µê³  í˜ì´ì§€ URL ë¦¬ìŠ¤íŠ¸
        self.job_urls = {
            "Frontend": f"{self.base_url}/software-engineer-frontend/?etype=INTERN#_filter",
            "Backend": f"{self.base_url}/software-engineer-backend/?etype=INTERN#_filter",
            "iOS": f"{self.base_url}/software-engineer-ios/?etype=INTERN#_filter",
            "Machine Learning": f"{self.base_url}/software-engineer-machine-learning/?etype=INTERN#_filter",
        }

    def get_html_selenium(self, url):
        """ Seleniumì„ ì‚¬ìš©í•˜ì—¬ JavaScriptê°€ ë Œë”ë§ëœ HTMLì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ """
        options = webdriver.ChromeOptions()
        options.binary_location = "/usr/bin/google-chrome"  # Chrome ì‹¤í–‰ ê²½ë¡œ ì§€ì •
        options.add_argument("--headless")  # GUI ì—†ì´ ì‹¤í–‰
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)

        driver.get(url)

        try:
            # **ğŸ“Œ ê³µê³  ë¦¬ìŠ¤íŠ¸ ì¡´ì¬ ì—¬ë¶€ ë¹ ë¥´ê²Œ í™•ì¸ (2ì´ˆ)**
            WebDriverWait(driver, timeout=2, poll_frequency=0.2).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".c-bQzyIt .c-jpGEAj li a"))
            )

            # **ğŸ“Œ ê³µê³ ê°€ ì—†ìœ¼ë©´ ë¹ ë¥´ê²Œ ì¢…ë£Œ**
            if not driver.find_elements(By.CSS_SELECTOR, ".c-bQzyIt .c-jpGEAj li a"):
                print(f"âš ï¸ {url}ì—ì„œ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë¹ ë¥´ê²Œ ì¢…ë£Œ)")
                driver.quit()
                return None

            # **ğŸ“Œ ê³µê³ ê°€ ìˆìœ¼ë©´ ë³¸ê²© í¬ë¡¤ë§ (ìµœëŒ€ 5ì´ˆ)**
            WebDriverWait(driver, timeout=5, poll_frequency=0.2).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".c-bQzyIt"))
            )
            html = driver.page_source

        except TimeoutException:
            print(f"âš ï¸ {url}ì—ì„œ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë¹ ë¥´ê²Œ ì¢…ë£Œ)")
            html = None  # ê³µê³  ì—†ìŒ ì¦‰ì‹œ ë°˜í™˜

        finally:
            driver.quit()
        
        return html

    def parse(self, html):
        """ ë‹¹ê·¼ë§ˆì¼“ ì¸í„´ ê³µê³ ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ """
        if not html:
            return []  # ê³µê³ ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        soup = BeautifulSoup(html, "html.parser")
        job_links = []

        job_elements = soup.select(".c-bQzyIt .c-jpGEAj li a")  # ëª¨ë“  ê³µê³  í•­ëª© ê°€ì ¸ì˜¤ê¸°

        if not job_elements:
            print("âš ï¸ ë‹¹ê·¼ë§ˆì¼“ ì¸í„´ ê³µê³ ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return []  # ê³µê³ ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        for a_tag in job_elements:
            href_value = a_tag.get("href")
            if not href_value:
                continue  # href ê°’ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ

            # ğŸ”¹ ê³µê³  URL ìƒì„±
            notice_url = f"https://about.daangn.com{href_value}"

            # ğŸ”¹ ê³µê³  ì œëª© ê°€ì ¸ì˜¤ê¸°
            title_tag = a_tag.select_one("h3.c-boyXyq")
            notice_title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

            # ğŸ”¹ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
            job_links.append({
                "notice_url": notice_url,
                "title": notice_title,
                "period": "ì •ë³´ ì—†ìŒ"
            })

        return job_links

    def crawl(self):
        """ ë‹¹ê·¼ë§ˆì¼“ ì¸í„´ ê³µê³  ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ """
        all_jobs = []

        for category, url in self.job_urls.items():
            print(f"ğŸ› ï¸ {category} ì¸í„´ ê³µê³  í¬ë¡¤ë§ ì¤‘...")

            html = self.get_html_selenium(url)

            if html:
                jobs = self.parse(html)
                all_jobs.extend(jobs)

        return all_jobs
