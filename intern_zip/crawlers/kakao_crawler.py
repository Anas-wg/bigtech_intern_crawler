from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
from .base_crawler import BaseCrawler

class KakaoCrawler(BaseCrawler):
    def __init__(self):
        super().__init__("https://careers.kakao.com")

    def get_html_selenium(self, url):
        """ Seleniumì„ ì‚¬ìš©í•˜ì—¬ JavaScriptê°€ ë Œë”ë§ëœ HTMLì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ """
        options = webdriver.ChromeOptions()
        options.binary_location = "/usr/bin/google-chrome"  # Chrome ì‹¤í–‰ ê²½ë¡œ ì§€ì •
        options.add_argument("--headless")  # GUI ì—†ì´ ì‹¤í–‰
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)

        driver.get(url)

        try:
            # **ğŸ“Œ ê³µê³  ë¦¬ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ 3ì´ˆë§Œ ëŒ€ê¸°**
            WebDriverWait(driver, timeout=3, poll_frequency=0.2).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".list_jobs"))
            )

            # **ğŸ“Œ ê³µê³  ë¦¬ìŠ¤íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸**
            if not driver.find_elements(By.CSS_SELECTOR, ".list_jobs > a"):
                print("âš ï¸ ê³µê³ ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ë¹ ë¥´ê²Œ ì¢…ë£Œ)")
                driver.quit()
                return None  # ê³µê³  ì—†ìŒ ì¦‰ì‹œ ë°˜í™˜

            html = driver.page_source

        except TimeoutException:
            print("âš ï¸ ê³µê³ ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ë¹ ë¥´ê²Œ ì¢…ë£Œ)")
            html = None  # ê³µê³  ì—†ìŒ ì¦‰ì‹œ ë°˜í™˜

        finally:
            driver.quit()
        
        return html

    def parse(self, html):
        """ ì¹´ì¹´ì˜¤ ì±„ìš© ê³µê³ ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ """
        if not html:
            return []  # ê³µê³ ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        soup = BeautifulSoup(html, "html.parser")
        job_links = []

        job_elements = soup.select(".list_jobs > a")  # ëª¨ë“  ê³µê³  í•­ëª© ê°€ì ¸ì˜¤ê¸°

        if not job_elements:
            print("âš ï¸ ê³µê³ ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return []  # ê³µê³ ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        for a_tag in job_elements:
            href_value = a_tag.get("href")
            if not href_value:
                continue  # href ê°’ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ

            # ğŸ”¹ ê³µê³  URL ìƒì„±
            notice_url = self.base_url + href_value.split("?")[0]

            # ğŸ”¹ ê³µê³  ì œëª© ê°€ì ¸ì˜¤ê¸°
            title_tag = a_tag.select_one(".area_info .wrap_info span h4")
            notice_title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

            # ğŸ”¹ ëª¨ì§‘ ê¸°ê°„ ê°€ì ¸ì˜¤ê¸°
            notice_period = None
            for dt, dd in zip(a_tag.select(".list_info dt"), a_tag.select(".list_info dd")):
                if dt.text.strip() == "ì˜ì…ë§ˆê°ì¼":  # "ëª¨ì§‘ ê¸°ê°„" ì°¾ê¸°
                    notice_period = dd.text.strip()
                    break  # ì°¾ìœ¼ë©´ ë°˜ë³µ ì¢…ë£Œ

            # ğŸ”¹ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
            job_links.append({
                "notice_url": notice_url,
                "title": notice_title,
                "period": notice_period
            })

        return job_links

    def crawl(self):
        """ ì¹´ì¹´ì˜¤ ì±„ìš© ê³µê³  ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ """
        url = f"{self.base_url}/jobs?skillSet=&part=TECHNOLOGY&company=ALL&keyword=&employeeType=3&page=1"
        html = self.get_html_selenium(url)  # Seleniumìœ¼ë¡œ ë™ì  ë Œë”ë§ëœ HTML ê°€ì ¸ì˜¤ê¸°
        if not html:
            return []  # ê³µê³ ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return self.parse(html)
