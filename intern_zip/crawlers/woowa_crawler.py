from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from .base_crawler import BaseCrawler

class WoowaCrawler(BaseCrawler):
    def __init__(self):
        super().__init__("https://career.woowahan.com")

        # ë°°ë‹¬ì˜ë¯¼ì¡± ì¸í„´ ê³µê³  URL
        self.job_url = f"{self.base_url}/?jobCodes=&employmentTypeCodes=BA002003&serviceSectionCodes=&careerPeriod=&keyword=&category=jobGroupCodes%3ABA005001#recruit-list"

    def get_html_selenium(self, url, wait_selector):
        """ Seleniumì„ ì‚¬ìš©í•˜ì—¬ JavaScriptê°€ ë Œë”ë§ëœ HTMLì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ """
        options = webdriver.ChromeOptions()
        options.binary_location = "/usr/bin/google-chrome"  # Chrome ì‹¤í–‰ ê²½ë¡œ ì§€ì •
        options.add_argument("--headless")  # GUI ì—†ì´ ì‹¤í–‰
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)

        driver.get(url)

        try:
            # í˜ì´ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, ".recruit-type-list")))
            html = driver.page_source
        except:
            print(f"âš ï¸ {url}ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë¹ ë¥´ê²Œ ì¢…ë£Œ)")
            html = None  # ê³µê³ ê°€ ì—†ì„ ê²½ìš° None ë°˜í™˜

        driver.quit()
        return html

    def parse_list_page(self, html):
        """ ë°°ë‹¬ì˜ë¯¼ì¡± ì¸í„´ ê³µê³  ëª©ë¡ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ """
        if not html:
            return []  # ê³µê³ ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        soup = BeautifulSoup(html, "html.parser")
        job_links = []

        job_elements = soup.select(".recruit-type-list li a.title")  # ëª¨ë“  ê³µê³  í•­ëª© ê°€ì ¸ì˜¤ê¸°

        if not job_elements:
            print("âš ï¸ ë°°ë‹¬ì˜ë¯¼ì¡± ì¸í„´ ê³µê³ ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return []  # ê³µê³ ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        for a_tag in job_elements:
            employment_type = a_tag.select_one(".flag-type span")  # "ì •ê·œì§", "ì¸í„´" ë“± ì •ë³´
            if not employment_type or "ì¸í„´" not in employment_type.text:
                continue  # ì¸í„´ ê³µê³ ê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ

            href_value = a_tag.get("href")  # ğŸ”¹ href ê°’ ê°€ì ¸ì˜¤ê¸°
            if not href_value:
                continue  # href ê°’ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ

            # ğŸ”¹ ê³µê³  URL ìƒì„± (base_url ê²°í•©)
            notice_url = f"{self.base_url}{href_value.split('?')[0]}"  # âœ… '?ë’¤ì˜ íŒŒë¼ë¯¸í„°' ì œê±°

            # ğŸ”¹ ê³µê³  ì œëª© ê°€ì ¸ì˜¤ê¸°
            title_tag = a_tag.select_one("p.fr-view")
            notice_title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

            # ğŸ”¹ ëª¨ì§‘ ê¸°ê°„ ê°€ì ¸ì˜¤ê¸° (ì—†ì„ ìˆ˜ë„ ìˆìŒ)
            notice_period = a_tag.select_one(".flag-type span:nth-child(2)")
            notice_period = notice_period.text.strip() if notice_period else "ì •ë³´ ì—†ìŒ"

            # ğŸ”¹ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
            job_links.append({
                "notice_url": notice_url,
                "title": notice_title,
                "period": notice_period,
            })

        return job_links

    def crawl(self):
        """ ë°°ë‹¬ì˜ë¯¼ì¡± ì¸í„´ ê³µê³  ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ """
        print("ğŸ› ï¸ ë°°ë‹¬ì˜ë¯¼ì¡± ì¸í„´ ê³µê³  í¬ë¡¤ë§ ì¤‘...")
        html = self.get_html_selenium(self.job_url, ".recruit-type-list")
        if not html:
            return []  # ê³µê³ ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return self.parse_list_page(html)
