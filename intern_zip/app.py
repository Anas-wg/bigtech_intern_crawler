import os
from flask import Flask, jsonify, render_template,send_from_directory
from collections import defaultdict
from datetime import datetime
from apscheduler.schedulers.background import BackgroundScheduler
from pymongo import MongoClient
from db.db import collection  # MongoDB ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
from crawlers.naver_crawler import NaverCrawler
from crawlers.kakao_crawler import KakaoCrawler
from crawlers.line_crawler import LineCrawler
from crawlers.woowa_crawler import WoowaCrawler
from crawlers.daangn_crawler import DaangnCrawler
from crawlers.toss_crawler import TossCrawler
from insert_to_mongo import insert_to_mongo
from dotenv import load_dotenv
load_dotenv()

IP = os.environ.get('AWSIP')

DB_PW = os.environ.get("DB_PW")

app = Flask(__name__, static_folder="/home/ubuntu/intern_crawler/intern_zip/static")
@app.route("/static/<path:filename>")
def static_files(filename):
    return send_from_directory("/home/ubuntu/intern_crawler/intern_zip/static", filename)

# ğŸ”¥ MongoDB ì„¤ì •
# client = MongoClient('localhost', 27017) #27017ë²ˆ í¬íŠ¸
client = MongoClient("mongodb://admin:{DB_PW}@{IP}", 27017)
db = client["job_scraper"]
meta_collection = db["meta"]  # âœ… ê°™ì€ job_scraper DB ì•ˆì—ì„œ meta ì»¬ë ‰ì…˜ ì‚¬ìš©

# ê¸°ë³¸ íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ (ê³µê³ ê°€ ì—†ì–´ë„ 0ìœ¼ë¡œ í‘œì‹œë˜ë„ë¡ ì„¤ì •)
COMPANIES = ["Naver", "Kakao", "LINE", "Woowa", "Daangn", "Toss"]

def update_last_crawl_time():
    """ í¬ë¡¤ë§ ì‹œê°„ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜ """
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    meta_collection.update_one({"type": "last_crawl"}, {"$set": {"timestamp": now}}, upsert=True)

def scheduled_crawl():
    """ ë§¤ì¼ ì•„ì¹¨ 8ì‹œì— ì‹¤í–‰ë  í¬ë¡¤ë§ ì‘ì—… """
    print(f"ğŸ” [ìŠ¤ì¼€ì¤„ë§] í¬ë¡¤ë§ ì‹œì‘: {datetime.now()}")

    # ê¸°ì¡´ DB ë°ì´í„° ì‚­ì œ
    collection.delete_many({})
    print("ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ!")

    # í¬ë¡¤ë§ ì‹¤í–‰ ë° DB ì €ì¥
    total_jobs = []
    for Crawler, name in [
        (NaverCrawler, "Naver"),
        (KakaoCrawler, "Kakao"),
        (LineCrawler, "LINE"),
        (WoowaCrawler, "Woowa"),
        (DaangnCrawler, "Daangn"),
        (TossCrawler, "Toss")
    ]:
        print(f"ğŸš€ {name} í¬ë¡¤ë§ ì¤‘...")
        crawler = Crawler()
        jobs = crawler.crawl()
        insert_to_mongo(name, jobs)
        total_jobs.extend(jobs)

    # í¬ë¡¤ë§ ì™„ë£Œ í›„ ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ ì €ì¥
    update_last_crawl_time()
    print(f"\nâœ… ëª¨ë“  ê³µê³  í¬ë¡¤ë§ & ì €ì¥ ì™„ë£Œ! ({len(total_jobs)}ê°œ)")

# ğŸ”¥ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ (Flask ì‹¤í–‰ ì¤‘ì—ë„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìë™ í¬ë¡¤ë§)
scheduler = BackgroundScheduler()
scheduler.add_job(scheduled_crawl, "cron", hour=8, minute=0)  # ë§¤ì¼ ì˜¤ì „ 8ì‹œ ì‹¤í–‰
scheduler.start()

@app.route("/")
def home():
    """ ê¸°ë³¸ ì›¹í˜ì´ì§€ ë Œë”ë§ """
    return render_template("index.html")

@app.route("/api/jobs/count", methods=["GET"])
def get_job_counts():
    """ íšŒì‚¬ë³„ ì±„ìš© ê³µê³  ê°œìˆ˜ ë°˜í™˜ (ì—†ìœ¼ë©´ 0) """
    pipeline = [
        {"$group": {"_id": "$company", "count": {"$sum": 1}}}
    ]
    counts = list(collection.aggregate(pipeline))

    # defaultdictì„ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ê°’ 0ìœ¼ë¡œ ì„¤ì •
    result = defaultdict(int, {entry["_id"]: entry["count"] for entry in counts})

    return jsonify({company: result[company] for company in COMPANIES})

@app.route("/api/last-crawl", methods=["GET"])
def get_last_crawl_time():
    """ ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê°„ì„ ë°˜í™˜í•˜ëŠ” API """
    last_crawl = meta_collection.find_one({"type": "last_crawl"})
    return jsonify({"last_crawl": last_crawl["timestamp"] if last_crawl else "ê¸°ë¡ ì—†ìŒ"})

@app.route("/api/jobs", methods=["GET"])
def get_jobs():
    """ ëª¨ë“  ì±„ìš© ê³µê³  ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜ (í¬ë¡¤ë§ ì—†ì´ ê¸°ì¡´ ë°ì´í„°ë§Œ ì¡°íšŒ) """
    jobs = list(collection.find({}, {"_id": 0}))
    return jsonify(jobs)

@app.route("/api/jobs/<company>", methods=["GET"])
def get_jobs_by_company(company):
    """ íŠ¹ì • íšŒì‚¬ì˜ ì±„ìš© ê³µê³  ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜ """
    jobs = list(collection.find({"company": company}, {"_id": 0}))
    return jsonify(jobs)

@app.route("/api/crawl", methods=["GET"])
def crawl_jobs():
    """ ì‚¬ìš©ìê°€ ìš”ì²­í•˜ë©´ í¬ë¡¤ë§ ì‹¤í–‰ í›„ MongoDB ì—…ë°ì´íŠ¸ """
    print("ğŸ” [ì‚¬ìš©ì ìš”ì²­] í¬ë¡¤ë§ ì‹œì‘...")

    # ê¸°ì¡´ DB ë°ì´í„° ì‚­ì œ
    collection.delete_many({})
    print("ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ!")

    # í¬ë¡¤ë§ ì‹¤í–‰ ë° DB ì €ì¥
    total_jobs = []
    for Crawler, name in [
        (NaverCrawler, "Naver"),
        (KakaoCrawler, "Kakao"),
        (LineCrawler, "LINE"),
        (WoowaCrawler, "Woowa"),
        (DaangnCrawler, "Daangn"),
        (TossCrawler, "Toss")
    ]:
        print(f"ğŸš€ {name} í¬ë¡¤ë§ ì¤‘...")
        crawler = Crawler()
        jobs = crawler.crawl()
        insert_to_mongo(name, jobs)
        total_jobs.extend(jobs)

    # í¬ë¡¤ë§ í›„ ê°œìˆ˜ ì •ë³´ë„ ì—…ë°ì´íŠ¸
    update_last_crawl_time()
    job_counts = get_job_counts().json

    print("\nâœ… ëª¨ë“  ê³µê³  í¬ë¡¤ë§ & ì €ì¥ ì™„ë£Œ!")

    return jsonify({"status": "success", "total_jobs": len(total_jobs), "job_counts": job_counts})

if __name__ == "__main__":
    app.run('0.0.0.0', debug=False, port=5000)
